{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Minutes to distributed GPU accelerated XGBoost using dmlc/xgboost's Dask API\n",
    "\n",
    "The [Dask API of DMLC XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html) enables distributed GPU accelerated XGBoost via distributed CUDA DataFrame, Dask-cuDF. A user may pass a reference to a distributed cuDF object, and start a training session over an entire cluster from Python. For a better understanding of this Dask API, please refer to [this article](https://medium.com/rapids-ai/a-new-official-dask-api-for-xgboost-e8b10f3d1eb7)\n",
    "\n",
    "Let's get started.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable NCCL P2P. Only necessary for versions of NCCL < 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NCCL_P2P_DISABLE=1\n"
     ]
    }
   ],
   "source": [
    "%env NCCL_P2P_DISABLE=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules and initialize the Dask-cuDF Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `LocalCUDACluster` from Dask-CUDA to instantiate the single-node cluster.\n",
    "\n",
    "A user may instantiate a Dask-cuDF cluster like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.33.227.157:36099</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.33.227.157:8787/status' target='_blank'>http://10.33.227.157:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>7</li>\n",
       "  <li><b>Cores: </b>7</li>\n",
       "  <li><b>Memory: </b>540.94 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.33.227.157:36099' processes=7 threads=7, memory=540.94 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "import cupy\n",
    "import dask\n",
    "import dask_cudf\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "import subprocess\n",
    "\n",
    "cmd = \"hostname --all-ip-addresses\"\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "IPADDR = str(output.decode()).split()[0]\n",
    "\n",
    "cluster = LocalCUDACluster(ip=IPADDR)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of `from dask_cuda import LocalCUDACluster`. [Dask-CUDA](https://github.com/rapidsai/dask-cuda) is a lightweight set of utilities useful for setting up a Dask cluster. These calls instantiate a Dask-cuDF cluster in a single node environment. To instantiate a multi-node Dask-cuDF cluster, a user must use `dask-scheduler` and `dask-cuda-worker`. These are utilities available at the command-line to launch the scheduler, and its associated workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Random Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `dask_cudf.DataFrame.query` to split the dataset into train-and-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000000\n",
    "npartitions = 8\n",
    "\n",
    "cdf = cudf.DataFrame({'x': cupy.random.randint(0, npartitions, size=size), 'y': cupy.random.normal(size=size)})\n",
    "ddf = dask_cudf.from_cudf(cdf,npartitions=npartitions)\n",
    "\n",
    "X_train = ddf.query('y < 0.5')\n",
    "Y_train = X_train[['x']]\n",
    "X_train = X_train[X_train.columns.difference(['x'])]\n",
    "\n",
    "X_test = ddf.query('y > 0.5')\n",
    "Y_test = X_test[['x']]\n",
    "X_test = X_test[X_test.columns.difference(['x'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Dask API for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DaskDMatrix object from our training input and target data using `xgb.dask.DaskMatrix`\n",
    "\n",
    "The DaskDMatrix constructor forces all lazy computation to materialize. To isolate the computation in DaskDMatrix from other lazy computations, one can use wait. Please note: this is optional. For more details, [refer here](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#why-is-the-initialization-of-daskdmatrix-so-slow-and-throws-weird-errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the XGBoost version used in this example\n",
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional: this wont return until all data is in GPU memory\n",
    "done = wait([X_train, Y_train])\n",
    "\n",
    "\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters and Train with XGBoost\n",
    "\n",
    "The wall time output below indicates how long it took your GPU cluster to train an XGBoost model over the training set.\n",
    "\n",
    "Use `dask_cudf.DataFrame.persist()` to ensure each GPU worker has ownership of data before training for optimal load-balance. Please note: this is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 190 ms, sys: 23.2 ms, total: 213 ms\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "  'num_rounds':   100,\n",
    "  'max_depth':    8,\n",
    "  'max_leaves':   2**8,\n",
    "  'tree_method':  'gpu_hist',\n",
    "  'objective':    'reg:squarederror',\n",
    "  'grow_policy':  'lossguide'\n",
    "}\n",
    "\n",
    "## Optional: persist training data into memory\n",
    "# X_train = X_train.persist()\n",
    "# Y_train = Y_train.persist()\n",
    "\n",
    "trained_model = xgb.dask.train(client,params, dtrain, num_boost_round=params['num_rounds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs for `xgb.dask.train`\n",
    "\n",
    "1. `client`: the `dask.distributed.Client`\n",
    "2. `params`: the training parameters for XGBoost. \n",
    "3. `dtrain`: an instance of `xgboost.dask.DaskDMatrix` containing our training input and target data.\n",
    "4. `num_boost_round=params['num_rounds']`: a specification on the number of boosting rounds for the training session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Predictions and the RMSE Metric for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distributed `xgb.dask.train` method returns a dictionary containing the resulting booster and evaluation history obtained from evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = trained_model[\"booster\"] # \"Booster\" is the trained model\n",
    "history = trained_model['history'] # \"History\" is a dictionary containing evaluation results \n",
    "\n",
    "booster.set_param({'predictor': 'gpu_predictor'}) #Setting this parameter to run predictions on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there are two different methods shown to get the predictions. Using `booster.inplace_predict()` and `xgb.dask.predict`. The inplace_predict can be faster if the prediction is run on the same workers that were used for training. Here are the details from the [doc](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.inplace_predict): \n",
    "\n",
    "Unlike predict method, inplace prediction does not cache the prediction result.\n",
    "Calling only inplace_predict in multiple threads is safe and lock free. But the safety does not hold when used in conjunction with other methods. E.g. you canâ€™t train the booster in one thread and perform prediction in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/rabhojwani/anaconda3/envs/rapids-env2/lib/python3.7/site-packages/distributed/worker.py:3378: UserWarning: Large object of size 16.23 MB detected in task graph: \n",
      "  [<function predict.<locals>.mapped_predict at 0x7f ... titions>, True]\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse value: 2.2921026956790693\n",
      "CPU times: user 277 ms, sys: 265 ms, total: 542 ms\n",
      "Wall time: 873 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Method 1: Using predict\n",
    "pred = xgb.dask.predict(client,booster, X_test)\n",
    "true = Y_test['x']\n",
    "pred = pred.map_partitions(lambda part: cudf.Series(part))\n",
    "pred = pred.reset_index(drop=True)\n",
    "true = true.reset_index(drop=True)\n",
    "\n",
    "\n",
    "squared_error = ((pred - true)**2)\n",
    "rmse = cupy.sqrt(squared_error.mean().compute())\n",
    "print('rmse value:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We mapped each partition of the result from `xgb.dask.predict` into `cudf.Series` to be able to substract it from `true` data. This is now taken care off inside `xgb.dask.predict` itself [[ref]](https://github.com/dmlc/xgboost/pull/5710) and will be available in the next stable version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to run prediction via `xgb.dask.predict`\n",
    "\n",
    "1. `client`: the `dask.distributed.Client`\n",
    "2. `booster`: the Booster produced by the XGBoost training session\n",
    "3. `X_test`: an instance of `dask_cudf.core.DataFrame` containing the data to be inferenced (acquire predictions)\n",
    "\n",
    "`pred` and `true` are instances of `dask_cudf.core.Series` object. We align the index of both the Series by resetting it through `reset_index`. To compute the root mean squared error(RMSE) we first compute difference squared between the `pred` and `true` series through the operation `squared_error = ((pred - true)**2)`\n",
    "\n",
    "Finally, the mean is computed by using an aggregator from the `dask_cudf` API. The entire computation is initiated via `.compute()`. We take the square-root of the result, leaving us with `rmse = cupy.sqrt(squared_error.mean().compute())`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse value: 2.2899321386939073\n",
      "CPU times: user 204 ms, sys: 117 ms, total: 321 ms\n",
      "Wall time: 490 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Method 2: Using inplace_predict\n",
    "pred = booster.inplace_predict(data=X_test.compute())\n",
    "true = Y_test['x']\n",
    "true = true.reset_index(drop=True).compute()\n",
    "\n",
    "squared_error = ((pred - true)**2)\n",
    "rmse = cupy.sqrt(squared_error.mean())\n",
    "print('rmse value:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to run prediction via `booster.predict`\n",
    "\n",
    "1. `data`: This parameter takes these foloowing datatypes: numpy.ndarray/scipy.sparse.csr_matrix/cupy.ndarray/cudf.DataFrame/pd.DataFrame. \n",
    "\n",
    "We run `X_test.compute()` to convert `dask_cudf.Dataframe` into `cudf.DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that `inplace_predict` takes `490 ms` while `predict` takes `873 ms`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
