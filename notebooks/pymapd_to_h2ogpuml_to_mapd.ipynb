{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Open Analytics Initiative: PyMapD to H2OGPUML to MapD\n",
    "\n",
    "### In this demo, we will train 4000 regularized linear regression models on the FIFA Football dataset, with the goal to predict the overall rating of the player, given different  feature sets (such as potential, finishing, strength, etc.)\n",
    "\n",
    "### The dataset has 180k rows, 42 cols, Integer/single-precision floating-point values, so it fits onto the GPU memory.\n",
    "\n",
    "### By using multiple GPUs, we are able to speed up this process significantly, and can train around 40 models per second (on Quadro - 6000 with 4 GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose to Store predictedictions in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# storage=0: Do not store the values in MapD\n",
    "# storage=1: Store the predicted values in MapD\n",
    "\n",
    "storage=1\n",
    "\n",
    "# storage=1: Predicted values will be copied to host memory in the process of inserting records in MapD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env \n",
    "import sys\n",
    "import os.path\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PWD = !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add MapD and Arrow schema path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raidStorage/wamsi/pygdf/notebooks/../thirdparty\n"
     ]
    }
   ],
   "source": [
    "thirdparty_path = os.path.join(PWD[0], '..', 'thirdparty')\n",
    "sys.path.append(thirdparty_path)\n",
    "print(thirdparty_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code below, if pygdf cannot be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pygdf_path = os.path.join(PWD[0], '..')\n",
    "# sys.path.append(pygdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PyMapD and PyGDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymapd\n",
    "import pygdf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup MapD Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established Connection(mapd://mapd:***@10.1.0.4:9998/mapd?protocol=binary)\n"
     ]
    }
   ],
   "source": [
    "db_name = 'mapd'\n",
    "user_name = 'mapd'\n",
    "passwd = 'HyperInteractive'\n",
    "hostname = '10.1.0.4'\n",
    "portno = 9998\n",
    "\n",
    "con = pymapd.connect(user=user_name, password=passwd, host=hostname, dbname=db_name, port=portno)\n",
    "print('Established {}'.format(con))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data from MapD to PyGDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No.of columns: 37\n"
     ]
    }
   ],
   "source": [
    "columns = '''overall_rating,rowid AS map_id,potential,CASE WHEN preferred_foot = 'left' THEN 1 WHEN preferred_foot = 'right' THEN 2 ELSE 0 END,crossing,finishing,heading_accuracy,short_passing,volleys,dribbling,curve,free_kick_accuracy,long_passing,ball_control,acceleration,sprint_speed,agility,reactions,balance,shot_power,jumping,stamina,strength,long_shots,aggression,interceptions,positioning,vision,penalties,marking,standing_tackle,sliding_tackle,gk_diving,gk_handling,gk_kicking,gk_positioning,gk_reflexes'''.strip()\n",
    "print('Total No.of columns: %d'%(len(columns.split(','))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select statement is : SELECT overall_rating,rowid AS map_id,potential,CASE WHEN preferred_foot = 'left' THEN 1 WHEN preferred_foot = 'right' THEN 2 ELSE 0 END,crossing,finishing,heading_accuracy,short_passing,volleys,dribbling,curve,free_kick_accuracy,long_passing,ball_control,acceleration,sprint_speed,agility,reactions,balance,shot_power,jumping,stamina,strength,long_shots,aggression,interceptions,positioning,vision,penalties,marking,standing_tackle,sliding_tackle,gk_diving,gk_handling,gk_kicking,gk_positioning,gk_reflexes FROM PLAYER_ATTRIBUTES WHERE overall_rating > 10 ORDER BY ID\n"
     ]
    }
   ],
   "source": [
    "query_select = '''SELECT {} FROM PLAYER_ATTRIBUTES WHERE overall_rating > 10 ORDER BY ID'''.format(columns)\n",
    "print('Select statement is : {}'.format(query_select))\n",
    "\n",
    "df = con.select_ipc_gpu(query_select, device_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure extracted data is float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[col] = df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_col = 'overall_rating'\n",
    "key_col = 'map_id'\n",
    "feature_col = set(df.columns) - set([response_col]) - set([key_col])\n",
    "\n",
    "num_col = set()\n",
    "cat_col = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if storage == 1:\n",
    "    # Create DF with key_col\n",
    "    from pygdf.dataframe import DataFrame\n",
    "    df_src = DataFrame()\n",
    "    df_src.add_column(key_col, df[key_col].astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinguish Categorical and Numerical columns by computing unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_val = {}\n",
    "for col in feature_col:\n",
    "    try:\n",
    "        val = df[col].unique_k(1000)\n",
    "    except ValueError:\n",
    "        # more than 1000 unique values\n",
    "        num_col.add(col)\n",
    "    else:\n",
    "        # value less than 1000\n",
    "        value = len(val)\n",
    "        if value <= 1:\n",
    "            del df[col]\n",
    "        elif 1 < value < 1000:\n",
    "            cat_col.add(col)\n",
    "        else:\n",
    "            num_col.add(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric Columns: Fill null values and Normalize data from 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in num_col:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "    assert df[col].null_count == 0\n",
    "    std = df[col].std()\n",
    "    # drop near constant columns\n",
    "    if not np.isfinite(std) or std < 1e-4:\n",
    "        del df[col]\n",
    "        print('drop near constant', col)\n",
    "    else:\n",
    "        df[col] = df[col].scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns: One-hot-encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in cat_col:\n",
    "    df[col] = df[col].fillna(-99)\n",
    "    cats = (df[col].unique_k(501))[1:]  # drop first\n",
    "    df = df.one_hot_encoding(col, prefix=col, cats=cats, dtype=np.float32)\n",
    "    del df[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add intercept column and unity weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows = len(df)\n",
    "df['intercept'] = np.ones(nrows, dtype=np.float64)\n",
    "df['weights'] = np.ones(nrows, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure response column is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[response_col] = df[response_col].fillna(df[response_col].mean())\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 60-30 split: training - testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60% of 183141 is 128198\n",
      "df_train has 128199 rows | df_test has 128199 rows\n"
     ]
    }
   ],
   "source": [
    "FRACTION=0.7\n",
    "\n",
    "n = int(len(df) * FRACTION)\n",
    "print('60% of {} is {}'.format(len(df), n))\n",
    "df_train = df.loc[:n]\n",
    "\n",
    "df_test = df.loc[n:]\n",
    "if storage == 1:\n",
    "    df_src = df_src.loc[n:]\n",
    "        \n",
    "    print('df_train has {} rows | df_test has {} rows'.format(len(df_train), len(df_train)))\n",
    "else:\n",
    "    print('df_train has {} rows | df_test has {} rows'.format(len(df_train), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make matrices from data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_mat = df_train.as_gpu_matrix(columns=df.columns[1:-1])\n",
    "train_result_mat = df_train.as_gpu_matrix(columns=[df.columns[0]])\n",
    "train_w_mat = df_train.as_gpu_matrix(columns=[df.columns[-1]])\n",
    "test_data_mat = df_test.as_gpu_matrix(columns=df.columns[1:-1])\n",
    "test_result_mat = df_test.as_gpu_matrix(columns=[df.columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128199, 3053)\n",
      "(128199, 1)\n",
      "(128199, 1)\n",
      "(54943, 3053)\n",
      "(54943, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_mat.shape)\n",
    "print(train_result_mat.shape)\n",
    "print(train_w_mat.shape)\n",
    "print(test_data_mat.shape)\n",
    "print(test_result_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make ctypes pointers to GPU matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_mat_ptr address 0x105cfe00000\n",
      "train_result_mat_ptr address 0x1062d30a600\n",
      "test_data_mat_ptr address 0x1062d400000\n",
      "test_result_mat_ptr address 0x105c9800000\n"
     ]
    }
   ],
   "source": [
    "train_data_mat_ptr = train_data_mat.device_ctypes_pointer\n",
    "train_result_mat_ptr = train_result_mat.device_ctypes_pointer\n",
    "print('train_data_mat_ptr address', hex(train_data_mat_ptr.value))\n",
    "print('train_result_mat_ptr address', hex(train_result_mat_ptr.value))\n",
    "\n",
    "test_data_mat_ptr = test_data_mat.device_ctypes_pointer\n",
    "test_result_mat_ptr = test_result_mat.device_ctypes_pointer\n",
    "print('test_data_mat_ptr address', hex(test_data_mat_ptr.value))\n",
    "print('test_result_mat_ptr address', hex(test_result_mat_ptr.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2OGPUML: Model training and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load H2OGPUML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2ogpuml\n",
    "from ctypes import *\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain = train_data_mat.device_ctypes_pointer\n",
    "ytrain = train_result_mat.device_ctypes_pointer\n",
    "xtest = test_data_mat.device_ctypes_pointer\n",
    "ytest = test_result_mat.device_ctypes_pointer\n",
    "wtrain = train_w_mat.device_ctypes_pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params for features, train values, shape\n",
    "n = train_data_mat.shape[1]\n",
    "mTrain = train_data_mat.shape[0]\n",
    "mValid = test_data_mat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper methods to display errors and running the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_errors(display, enet, givefullpath):\n",
    "    err_best = enet.error_best\n",
    "    alphas_best = enet.alphas_best\n",
    "    lambdas_best = enet.lambdas_best\n",
    "    tols_best = enet.tols_best\n",
    "\n",
    "    if givefullpath == 1:\n",
    "        err_full = enet.error_full\n",
    "        alphas_full = enet.alphas_full\n",
    "        lambdas_full = enet.lambdas_full\n",
    "        tols_full = enet.tols_full\n",
    "\n",
    "    loss = 'RMSE'\n",
    "\n",
    "    if enet.family == 'logistic':\n",
    "        loss = \"LOGLOSS\"\n",
    "\n",
    "    if display == 1:\n",
    "        # Display most important metrics\n",
    "        if givefullpath == 1:\n",
    "            print('Alphas full path: {}'.format(alphas_full))\n",
    "            print('Lambdas full path: {}'.format(lambdas_best))\n",
    "            print('Tols full path: {}'.format(tols_full))\n",
    "            print('RMSE full path: {}'.format(err_full))\n",
    "\n",
    "        print('Alphas: {}'.format(alphas_best))\n",
    "        print('Lambdas: {}'.format(lambdas_best))\n",
    "        print('Tols: {}'.format(tols_best))\n",
    "        print('{}: {}'.format(loss, err_best))\n",
    "\n",
    "    return err_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_glm(nFolds, nAlphas, nLambdas, xtrain, ytrain, xtest, ytest, wtrain):\n",
    "\n",
    "   # params for features, train values, shape\n",
    "    n = train_data_mat.shape[1]\n",
    "    mTrain = train_data_mat.shape[0]\n",
    "    mValid = test_data_mat.shape[0]\n",
    "\n",
    "    maxNGPUS = int(subprocess.check_output(\"nvidia-smi -L | wc -l\", shell=True))\n",
    "    print(\"Maximum Number of GPUS:\", maxNGPUS)\n",
    "    nGPUs = maxNGPUS  # choose all GPUs\n",
    "\n",
    "    print(\"No. of Features=%d mTrain=%d mValid=%d\" % (n, mTrain, mValid))\n",
    "\n",
    "    # Order of data\n",
    "    fortran = 1\n",
    "    print(\"fortran=%d\" % (fortran))\n",
    "\n",
    "    sourceme = 0\n",
    "    sourceDev = 0\n",
    "    nThreads = None  # default value, algo handles it.\n",
    "    precision = 0\n",
    "    # variables\n",
    "    a, b = c_void_p(xtrain.value), c_void_p(ytrain.value)\n",
    "    c, d = c_void_p(xtest.value), c_void_p(ytest.value)\n",
    "    e = c_void_p(wtrain.value)\n",
    "\n",
    "    #print(a, b, c, d, e)\n",
    "\n",
    "    print(\"Setting up Solver\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    Solver = h2ogpuml.GLM\n",
    "    enet = Solver(n_threads=nThreads, n_gpus=nGPUs, order='c' if fortran else 'r', intercept=intercept, \\\n",
    "                  lambda_min_ratio=lambda_min_ratio, n_lambdas=nLambdas, n_folds=nFolds, n_alphas=nAlphas, \\\n",
    "                  verbose=5, give_full_path=givefullpath)\n",
    "\n",
    "    print(\"Solving\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    enet.fit_ptr(sourceDev, mTrain, n, mValid, precision, 'c' if fortran else 'r', a, b, c, d, e, give_full_path=givefullpath)\n",
    "\n",
    "    print(\"Done Solving\\n\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print('Predicting')\n",
    "    sys.stdout.flush()\n",
    "    pred_val = enet.predict_ptr(c, d, give_full_path=givefullpath)\n",
    "\n",
    "    print('Done Predicting')\n",
    "    sys.stdout.flush()\n",
    "    print('predicted values:\\n', pred_val)\n",
    "\n",
    "    loss = get_errors(display, enet, givefullpath)\n",
    "\n",
    "    if write == 0:\n",
    "        os.system('rm -f rmse.txt; rm -f pred*.txt; rm -f varimp.txt; rm -f me*.txt; rm -f stats.txt')\n",
    "\n",
    "    return pred_val, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Parameters for building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intercept = True\n",
    "lambda_min_ratio = 1e-9\n",
    "nAlphas = 8\n",
    "nLambdas = 100\n",
    "nFolds = 5\n",
    "\n",
    "givefullpath = 0\n",
    "write = 0 # Write values to file\n",
    "display = 1 # To print the errors\n",
    "\n",
    "#run the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Number of GPUS: 4\n",
      "No. of Features=3053 mTrain=128199 mValid=54943\n",
      "fortran=1\n",
      "Setting up Solver\n",
      "\n",
      "Warning: Cannot create a H2OGPUML Elastic Net CPU Solver instance without linking Python module to a compiled H2OGPUML >CPU library Use GPU or re-run setup.py\n",
      "Using GPU GLM solver with 4 GPUs\n",
      "Solving\n",
      "Done Solving\n",
      "\n",
      "Predicting\n",
      "Done Predicting\n",
      "predicted values:\n",
      " [[ 72.16277313  75.32152557  60.04825592 ...,  73.63434601  73.80071259\n",
      "   74.88893127]\n",
      " [ 72.07253265  75.36482239  59.76856232 ...,  73.49241638  73.66304779\n",
      "   74.91437531]\n",
      " [ 71.96218872  75.38594818  59.44565201 ...,  73.38622284  73.49422455\n",
      "   75.00521088]\n",
      " ..., \n",
      " [ 71.91691589  75.90367889  60.14600372 ...,  73.14762115  73.24468231\n",
      "   75.05224609]\n",
      " [ 72.06842041  75.99766541  60.93129349 ...,  73.09658813  73.19271088\n",
      "   75.0697937 ]\n",
      " [ 71.62844849  75.3739624   54.77607346 ...,  73.86998749  73.97012329\n",
      "   76.21363831]]\n",
      "Alphas: [[ 0.        ]\n",
      " [ 0.14285715]\n",
      " [ 0.2857143 ]\n",
      " [ 0.42857143]\n",
      " [ 0.5714286 ]\n",
      " [ 0.71428573]\n",
      " [ 0.85714287]\n",
      " [ 1.        ]]\n",
      "Lambdas: [[ 519.39978027]\n",
      " [ 543.37701416]\n",
      " [ 519.39978027]\n",
      " [ 610.78009033]\n",
      " [ 664.31756592]\n",
      " [ 752.99822998]\n",
      " [ 964.7746582 ]\n",
      " [ 172.05328369]]\n",
      "Tols: [[ 0.001]\n",
      " [ 0.001]\n",
      " [ 0.001]\n",
      " [ 0.001]\n",
      " [ 0.001]\n",
      " [ 0.001]\n",
      " [ 0.001]\n",
      " [ 0.001]]\n",
      "RMSE: [[ 3.17579055  3.11931515  3.18330431]\n",
      " [ 3.14654374  3.23858285  3.17401481]\n",
      " [ 3.1638577   3.25120783  3.17956352]\n",
      " [ 3.24574804  3.31208944  3.24335122]\n",
      " [ 3.28871894  3.34103274  3.2748754 ]\n",
      " [ 3.3528595   3.38972282  3.3327744 ]\n",
      " [ 3.50115609  3.51194668  3.47279668]\n",
      " [ 3.09059548  3.20231366  3.13546848]]\n"
     ]
    }
   ],
   "source": [
    "pred_val, loss = run_glm(nFolds, nAlphas, nLambdas, xtrain, ytrain, xtest, ytest, wtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper methods to store Predictions in MapD DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from thrift.protocol import TBinaryProtocol\n",
    "from thrift.protocol import TJSONProtocol\n",
    "from thrift.transport import TSocket\n",
    "from thrift.transport import THttpClient\n",
    "from thrift.transport import TTransport\n",
    "\n",
    "from mapd import MapD\n",
    "from mapd import ttypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_rows(df_lst):\n",
    "    row_lst = []\n",
    "    for k, v in df_lst:\n",
    "        row = MapD.TStringRow()\n",
    "        row.cols = [MapD.TStringValue(str(k)), MapD.TStringValue(str(v))]\n",
    "        row_lst.append(row)\n",
    "\n",
    "    return row_lst\n",
    "\n",
    "def get_index(loss):\n",
    "    first = True\n",
    "    prev_val = None\n",
    "\n",
    "    for i in range(len(loss)):\n",
    "        curr_val = loss[i][2]\n",
    "        if first:\n",
    "            prev_val = curr_val\n",
    "            ind = i\n",
    "            first = False\n",
    "        elif curr_val - prev_val < 0:\n",
    "            prev_val = curr_val\n",
    "            ind = i\n",
    "    return ind\n",
    "\n",
    "def insert_results(pred_val, n_table, n_predcol, loss):\n",
    "    ind = get_index(loss)\n",
    "    \n",
    "    # predicted values from dataframe to list\n",
    "    df_pred = pd.DataFrame(pred_val[7][np.newaxis][0].T)\n",
    "    df_src_test = df_src.to_pandas()\n",
    "    df_src_test.reset_index(inplace=True,drop=True)\n",
    "    df_src_test[n_predcol] = df_pred\n",
    "    df_lst = df_src_test.values.tolist()\n",
    "\n",
    "    # Insert predicted values in MapD\n",
    "    row_list = format_rows(df_lst)\n",
    "\n",
    "    msg = client.load_table(session, n_table, row_list)\n",
    "\n",
    "    print(\"Predicted values inserted in MapD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_client(host_or_uri, port, http):\n",
    "  if http:\n",
    "    transport = THttpClient.THttpClient(host_or_uri)\n",
    "    protocol = TJSONProtocol.TJSONProtocol(transport)\n",
    "  else:\n",
    "    socket = TSocket.TSocket(host_or_uri, port)\n",
    "    transport = TTransport.TBufferedTransport(socket)\n",
    "    protocol = TBinaryProtocol.TBinaryProtocol(transport)\n",
    "\n",
    "  client = MapD.Client(protocol)\n",
    "  transport.open()\n",
    "  return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values inserted in MapD\n",
      "Join Query is:  SELECT a.overall_rating,b.overall_rating_Pred FROM Player_Attributes a, prediction_table b WHERE a.rowid = b.map_id\n"
     ]
    }
   ],
   "source": [
    "if storage == 1:\n",
    "    \n",
    "    client = get_client(hostname, portno, False)\n",
    "    session = client.connect(user_name, passwd, db_name)\n",
    "\n",
    "    n_table = 'prediction_table'\n",
    "    n_predcol = response_col+'_Pred'\n",
    "    \n",
    "    query_pred_drop = 'DROP TABLE IF EXISTS {};'.format(n_table)\n",
    "    query_pred_create = 'CREATE TABLE {}({} BIGINT NOT NULL, {} INT);'.format(n_table, key_col, n_predcol)\n",
    "\n",
    "    # initialize a cursor to mapd\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute(query_pred_drop)\n",
    "    cur.execute(query_pred_create)\n",
    "\n",
    "    insert_results(pred_val, n_table, n_predcol, loss)\n",
    "    \n",
    "    query_join = '''SELECT a.{},b.{} FROM Player_Attributes a, {} b WHERE a.rowid = b.{}'''.format(response_col,n_predcol,n_table,key_col)\n",
    "    \n",
    "    print('Join Query is: ',query_join)\n",
    "    \n",
    "    df_join = con.select_ipc(query_join)\n",
    "    \n",
    "    cur.close()\n",
    "    con.close()\n",
    "else:\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall_rating  overall_rating_Pred\n",
      "0              72                   72\n",
      "1              72                   72\n",
      "2              72                   73\n",
      "3              74                   75\n",
      "4              67                   67\n",
      "5              67                   67\n",
      "6              67                   67\n",
      "7              73                   72\n",
      "8              75                   74\n",
      "9              78                   75\n"
     ]
    }
   ],
   "source": [
    "if storage == 1:     \n",
    "    pprint(df_join.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg]",
   "language": "python",
   "name": "conda-env-pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
