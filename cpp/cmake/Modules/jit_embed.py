# SPDX-FileCopyrightText: Copyright (c) 2026, NVIDIA CORPORATION. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
import argparse
import hashlib
import logging
import os
from typing import NamedTuple, Self

import lz4.frame
import yaml

LIST_LINE_WIDTH = 32
NAMESPACE_PREFIX = "jit_"


# TODO: write a schema validator for the input YAML


### json schema

"""entries
[
    $id: {
        "type": "sources",
        "sources": [
            {
                "file": string,
                "dest": string
            }
        ],
        "include_directories": [string]
        "compression": string
    },
    $id: {
        "type": "strings",
        "strings": list[string]
    },
    $id: {
        "type": "blobs",
        "blobs": [
            {
                "file": string,
                "dest": string
            }
        ],
        "compression": string
    }
]
"""


PREAMBLE = f"""
/// Auto-generated by jit_embed.py. Do not edit directly.
#pragma once

extern "C" {{

typedef struct {NAMESPACE_PREFIX}bytes_t {{
    unsigned char const * data;
    unsigned long size;
}} {NAMESPACE_PREFIX}bytes_t;

typedef struct {NAMESPACE_PREFIX}byte_range_t {{
    unsigned long offset;
    unsigned long size;
}} {NAMESPACE_PREFIX}byte_range_t;

typedef struct {NAMESPACE_PREFIX}bytes_array_t {{
    {NAMESPACE_PREFIX}bytes_t bytes;
    {NAMESPACE_PREFIX}byte_range_t const * ranges;
    unsigned long num_ranges;
}} {NAMESPACE_PREFIX}bytes_array_t;

}}

"""

logger = logging.getLogger(__name__)


def list_string(strings: list[str]) -> str:
    lines = []
    for i in range(0, len(strings), LIST_LINE_WIDTH):
        line = ", ".join(strings[i : i + LIST_LINE_WIDTH])
        lines.append(line)
    return ",\n".join(lines)


def byte_hex_string(value: int) -> str:
    return f"0x{value:02X}"


class CXXVarDecl(NamedTuple):
    id: str
    expr: str

    @staticmethod
    def of_bytes(id: str, data: bytes, alignment: int) -> Self:
        byte_array = list_string([byte_hex_string(b) for b in data])
        expr = f"""alignas({alignment}) static unsigned char const {id}[{len(data)}] = {{
{byte_array}
}};"""
        return CXXVarDecl(id=id, expr=expr)

    @staticmethod
    def of_size(id: str, size: int) -> Self:
        expr = f"static long const {id} = {size}L;"
        return CXXVarDecl(id=id, expr=expr)

    def decl(self: Self) -> str:
        return f"""{self.expr}"""


class CXXSizeDecl(NamedTuple):
    id: str
    size: int

    @staticmethod
    def of_size(id: str, size: int) -> Self:
        return CXXSizeDecl(id=id, size=size)

    def var(self: Self) -> CXXVarDecl:
        expr = f"static unsigned long const {self.id} = {self.size}UL;"
        return CXXVarDecl(id=self.id, expr=expr)


class CXXSizeArrayDecl(NamedTuple):
    id: str
    sizes: list[int]

    @staticmethod
    def of_sizes(id: str, sizes: list[int]) -> Self:
        return CXXSizeArrayDecl(id=id, sizes=sizes)

    def var(self: Self) -> CXXVarDecl:
        size_array = list_string([f"{size}UL" for size in self.sizes])
        expr = f"""static unsigned long const {self.id}[{len(self.sizes)}] = {{
{size_array}
}};"""
        return CXXVarDecl(id=self.id, expr=expr)


class CXXBytesDecl(NamedTuple):
    id: str
    data: bytes
    alignment: int
    num_null_terminators: int

    @staticmethod
    def of_bytes(
        id: str, data: bytes, alignment: int, num_null_terminators: int
    ) -> Self:
        return CXXBytesDecl(
            id=id,
            data=data,
            alignment=alignment,
            num_null_terminators=num_null_terminators,
        )

    def var(self: Self) -> CXXVarDecl:
        # exclude null terminator from length
        size_decl = CXXVarDecl.of_size(
            id=f"{self.id}_size", size=len(self.data)
        )

        data = self.data + b"\0" * self.num_null_terminators

        data_decl = CXXVarDecl.of_bytes(
            id=f"{self.id}_data", data=data, alignment=self.alignment
        )

        return CXXVarDecl(
            id=self.id,
            expr=f"""
{data_decl.decl()}

{size_decl.decl()}


static {NAMESPACE_PREFIX}bytes_t const {self.id} = {{
    .data = {data_decl.id},
    .size = {size_decl.id}
}};
""",
        )


class CXXRangesDecl(NamedTuple):
    id: str
    ranges: list[tuple[int, int]]  # list of (offset, size)

    @staticmethod
    def of_ranges(id: str, ranges: list[tuple[int, int]]) -> Self:
        return CXXRangesDecl(id=id, ranges=ranges)

    def var(self: Self) -> CXXVarDecl:
        ranges_str = [
            f"{{{offset}UL, {size}UL}}" for offset, size in self.ranges
        ]

        ranges_str_formatted = list_string(ranges_str)

        expr = f"""static {NAMESPACE_PREFIX}byte_range_t const {self.id}[{len(self.ranges)}] = {{
{ranges_str_formatted}
}};"""
        return CXXVarDecl(id=self.id, expr=expr)


class CXXArrayOfBytesDecl(NamedTuple):
    id: str
    data: bytes
    alignment: int
    ranges: list[tuple[int, int]]  # list of (offset, size)

    @staticmethod
    def of_byte_ranges(
        id: str,
        data: bytes,
        ranges: list[tuple[int, int]],
        alignment: int,
    ) -> Self:
        return CXXArrayOfBytesDecl(
            id=id,
            data=data,
            alignment=alignment,
            ranges=ranges,
        )

    def var(self: Self) -> CXXVarDecl:
        bytes_decl = CXXBytesDecl.of_bytes(
            id=f"{self.id}_bytes",
            data=self.data,
            alignment=self.alignment,
            num_null_terminators=0,
        )

        ranges_decl = CXXRangesDecl.of_ranges(
            id=f"{self.id}_ranges", ranges=self.ranges
        )

        expr = f"""
{bytes_decl.var().decl()}

{ranges_decl.var().decl()}

static {NAMESPACE_PREFIX}bytes_array_t const {self.id} = {{
    .bytes = {bytes_decl.id},
    .ranges = {ranges_decl.id},
    .num_ranges = {len(self.ranges)}
}};
"""

        return CXXVarDecl(
            id=self.id,
            expr=expr,
        )


class CXXBinEmbedDecl(NamedTuple):
    id: str

    @staticmethod
    def of(id: str) -> Self:
        return CXXBinEmbedDecl(id=id)

    def var(self: Self) -> CXXVarDecl:
        return CXXVarDecl(
            id=self.id,
            expr=f"""
 extern unsigned char const {self.id}_begin[];
 extern unsigned char const {self.id}_end[];

static {NAMESPACE_PREFIX}bytes_t const {self.id} = {{
    .data = {self.id}_begin,
    .size = (unsigned long)({self.id}_end - {self.id}_begin)
}};

 """,
        )


class AsmEmbedDecl(NamedTuple):
    id: str
    file: str

    @staticmethod
    def of_file(id: str, file: str) -> Self:
        return AsmEmbedDecl(id=id, file=file)

    def decl(self: Self) -> str:
        return f"""
.section .rodata
.global {self.id}_begin
.global {self.id}_end
{self.id}_begin:
.incbin "{self.file}"
{self.id}_end:
"""


def merge_bytes_with_null_terminators(
    bytes_lists: list[bytes],
) -> tuple[bytes, list[tuple[int, int]]]:
    merged: bytes = bytes()
    ranges: list[tuple[int, int]] = []

    for byte_data in bytes_lists:
        ranges.append((len(merged), len(byte_data)))
        merged += byte_data + b"\0"

    return merged, ranges


class EmbedOutput(NamedTuple):
    cxx_header: str | None
    cxx_source: str | None
    asm_source: str | None
    bin_file_name: str | None
    bin_file_data: bytes | None
    hash: bytes


def generate_cxx_strings_data(id: str, strings: list[str]) -> EmbedOutput:
    data, ranges = merge_bytes_with_null_terminators(
        [s.encode("utf-8") for s in strings]
    )

    sha = hashlib.sha256()
    sha.update(data)
    hash = sha.digest()

    arrays_decl = CXXArrayOfBytesDecl.of_byte_ranges(
        id=f"{id}",
        data=data,
        alignment=1,
        ranges=ranges,
    )

    cxx_header = f"""
{arrays_decl.var().decl()}
"""

    return EmbedOutput(
        cxx_header=cxx_header,
        cxx_source=None,
        asm_source=None,
        bin_file_name=None,
        bin_file_data=None,
        hash=hash,
    )


def load_file_bytes(file_path: str) -> bytes:
    with open(file_path, "rb") as f:
        return f.read()


def generate_cxx_source_files_data(
    id: str,
    file_paths: list[str],
    dests: list[str],
    include_directories: list[str],
    compression: str,
) -> EmbedOutput:
    uncompressed_files_bytes, files_ranges = merge_bytes_with_null_terminators(
        [load_file_bytes(p) for p in file_paths]
    )

    assert compression in ("none", "lz4"), "Invalid compression type"
    compress = compression != "none"

    compressed_files_bytes = (
        lz4.frame.compress(
            uncompressed_files_bytes,
            compression_level=lz4.frame.COMPRESSIONLEVEL_MAX,
        )
        if compress
        else None
    )

    if compress:
        logger.info(
            f"{id}'s uncompressed size is {len(uncompressed_files_bytes)} bytes, compressed size is {len(compressed_files_bytes)} bytes"
        )

    merged_dests_bytes, merged_dests_ranges = (
        merge_bytes_with_null_terminators([d.encode("utf-8") for d in dests])
    )

    merged_include_directories_bytes, merged_include_directories_ranges = (
        merge_bytes_with_null_terminators(
            [d.encode("utf-8") for d in include_directories]
        )
    )

    # compute combined sha256 hash of all files
    sha = hashlib.sha256()
    sha.update(uncompressed_files_bytes)
    sha.update(merged_dests_bytes)
    sha.update(merged_include_directories_bytes)
    sha.update(compression.encode("utf-8"))

    hash: bytes = sha.digest()

    file_destinations_decls: CXXArrayOfBytesDecl = (
        CXXArrayOfBytesDecl.of_byte_ranges(
            id=f"{id}_file_destinations",
            data=merged_dests_bytes,
            ranges=merged_dests_ranges,
            alignment=1,
        )
    )

    binary_file_name = f"{id}_binary.bin"

    binary_decl: CXXBinEmbedDecl = CXXBinEmbedDecl.of(id=f"{id}_binary")

    binary_size_decl: CXXSizeDecl = CXXSizeDecl.of_size(
        id=f"{id}_uncompressed_size", size=len(uncompressed_files_bytes)
    )

    binary_embed_decl: AsmEmbedDecl = AsmEmbedDecl.of_file(
        id=f"{id}_binary", file=binary_file_name
    )

    binary_ranges_decl = CXXRangesDecl.of_ranges(
        id=f"{id}_ranges", ranges=files_ranges
    )

    include_directories_decls: CXXArrayOfBytesDecl = (
        CXXArrayOfBytesDecl.of_byte_ranges(
            id=f"{id}_include_directories",
            data=merged_include_directories_bytes,
            ranges=merged_include_directories_ranges,
            alignment=1,
        )
    )

    cxx_header = f"""
{file_destinations_decls.var().decl()}

{binary_decl.var().decl()}

{binary_size_decl.var().decl()}

{binary_ranges_decl.var().decl()}

{include_directories_decls.var().decl()}

"""

    asm_source = f"""
{binary_embed_decl.decl()}
"""

    return EmbedOutput(
        cxx_header=cxx_header,
        cxx_source=None,
        asm_source=asm_source,
        bin_file_name=binary_file_name,
        bin_file_data=compressed_files_bytes
        if compress
        else uncompressed_files_bytes,
        hash=hash,
    )


def generate_cxx_blobs_data(
    id: str, blob_paths: list[str], dests: list[str], compression: str
) -> EmbedOutput:
    uncompressed_blob_bytes, blob_ranges = merge_bytes_with_null_terminators(
        [load_file_bytes(p) for p in blob_paths]
    )

    assert compression in ("none", "lz4"), "Invalid compression type"
    compress = compression != "none"

    compressed_blob_bytes = (
        lz4.frame.compress(
            uncompressed_blob_bytes,
            compression_level=lz4.frame.COMPRESSIONLEVEL_MAX,
        )
        if compress
        else None
    )

    merged_dests_bytes, merged_dests_ranges = (
        merge_bytes_with_null_terminators([d.encode("utf-8") for d in dests])
    )

    if compress:
        logger.info(
            f"{id}'s uncompressed size is {len(uncompressed_blob_bytes)} bytes, compressed size is {len(compressed_blob_bytes)} bytes"
        )

    # compute combined sha256 hash of all files
    sha = hashlib.sha256()
    sha.update(uncompressed_blob_bytes)
    sha.update(merged_dests_bytes)
    sha.update(compression.encode("utf-8"))

    hash: bytes = sha.digest()

    file_destinations_decls: CXXArrayOfBytesDecl = (
        CXXArrayOfBytesDecl.of_byte_ranges(
            id=f"{id}_file_destinations",
            data=merged_dests_bytes,
            ranges=merged_dests_ranges,
            alignment=1,
        )
    )

    binary_file_name = f"{id}_binary.bin"

    binary_decl: CXXBinEmbedDecl = CXXBinEmbedDecl.of(id=f"{id}_binary")

    binary_size_decl: CXXSizeDecl = CXXSizeDecl.of_size(
        id=f"{id}_uncompressed_size", size=len(uncompressed_blob_bytes)
    )

    binary_embed_decl: AsmEmbedDecl = AsmEmbedDecl.of_file(
        id=f"{id}_binary", file=binary_file_name
    )

    binary_ranges_decl = CXXRangesDecl.of_ranges(
        id=f"{id}_ranges", ranges=blob_ranges
    )

    cxx_header = f"""

{file_destinations_decls.var().decl()}

{binary_decl.var().decl()}

{binary_size_decl.var().decl()}

{binary_ranges_decl.var().decl()}
"""

    asm_source = f"""
{binary_embed_decl.decl()}
"""

    return EmbedOutput(
        cxx_header=cxx_header,
        cxx_source=None,
        asm_source=asm_source,
        bin_file_name=binary_file_name,
        bin_file_data=compressed_blob_bytes
        if compress
        else uncompressed_blob_bytes,
        hash=hash,
    )


def generate_embed(
    id: str, entries: dict[str, dict[str, dict]], output_dir: str
):
    outputs: list[EmbedOutput] = []
    sha = hashlib.sha256()

    for entry_id, entry_value in entries.items():
        entry_type = entry_value["type"]

        if entry_type == "sources":
            sources: list[dict] = entry_value["sources"]
            file_paths = [s["file"] for s in sources]
            dests = [s["dest"] for s in sources]
            include_directories: list[str] = entry_value["include_directories"]
            compression = entry_value["compression"]
            output = generate_cxx_source_files_data(
                entry_id, file_paths, dests, include_directories, compression
            )
            sha.update(output.hash)
            outputs.append(output)

        elif entry_type == "strings":
            options: list[str] = entry_value["strings"]
            output = generate_cxx_strings_data(entry_id, options)
            sha.update(output.hash)
            outputs.append(output)

        elif entry_type == "blobs":
            blobs: list[str] = entry_value["blobs"]
            file_paths = [s["file"] for s in blobs]
            dests = [s["dest"] for s in blobs]
            compression = entry_value["compression"]
            output = generate_cxx_blobs_data(
                entry_id, file_paths, dests, compression
            )
            sha.update(output.hash)
            outputs.append(output)

        else:
            raise ValueError(f"Unknown type: {entry_type}")

    hash = sha.digest()

    hash_decl: CXXBytesDecl = CXXBytesDecl.of_bytes(
        id=f"{id}_hash",
        data=hash,
        alignment=1,
        num_null_terminators=0,
    )

    cxx_header = f"""
{PREAMBLE}

extern "C" {{


{hash_decl.var().decl()}

{"\n\n".join([output.cxx_header if output.cxx_header is not None else "" for output in outputs])}

}}

"""

    asm_source = f"""
{"\n\n".join([output.asm_source if output.asm_source is not None else "" for output in outputs])}

.section .note.GNU-stack,"",@progbits
"""

    os.makedirs(output_dir, exist_ok=True)

    with open(f"{output_dir}/embed.hpp", "w") as f:
        f.write(cxx_header)

    with open(f"{output_dir}/embed.s", "w") as f:
        f.write(asm_source)

    for output in outputs:
        if output.bin_file_name and output.bin_file_data:
            with open(f"{output_dir}/{output.bin_file_name}", "wb") as f:
                f.write(output.bin_file_data)


# Usage: embed.py --id <id> --input <input file> --output-dir <output dir>
def main():
    parser = argparse.ArgumentParser(
        description="Embed headers, options, or binary blobs into C++ source code."
    )

    parser.add_argument(
        "--id",
        type=str,
        required=True,
        help="Identifier for the output",
    )

    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="YAML description of what to embed",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        required=True,
        help="Output directory for generated files",
    )

    args = parser.parse_args()

    with open(args.input, "rb") as f:
        description = yaml.safe_load(f)

    generate_embed(args.id, description, args.output_dir)


if __name__ == "__main__":
    main()
