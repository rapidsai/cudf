# Copyright (c) 2021, NVIDIA CORPORATION.

from __future__ import annotations
import cupy as cp
from warnings import warn
from cudf._lib.nvtext.subword_tokenize import (
    subword_tokenize_inmem_hash as cpp_subword_tokenize,
    Hashed_Vocabulary as cpp_hashed_vocabulary,
)


def _cast_to_appripate_type(ar, cast_type):
    if cast_type == "cp":
        return ar

    if cast_type == "pt":
        from torch.utils.dlpack import from_dlpack

    elif cast_type == "tf":
        from tf.experimental.dlpack import from_dlpack

    return from_dlpack(ar.astype("int32").toDlpack())


class SubwordTokenizer:
    """
    Run CUDA BERT subword tokenizer on cuDF strings column.
    Encodes words to token ids using vocabulary from a pretrained
    tokenizer.
    This function requires about 21x the number of character bytes
    in the input strings column as working memory.

    Parameters
    ----------
    hash_file : str
        Path to hash file containing vocabulary of words with token-ids.
        This can be created from the raw vocabulary
        using the ``cudf.utils.hash_vocab_utils.hash_vocab`` function

    do_lower : bool, Default is True
        If set to true, original text will be lowercased before encoding.

    Returns
    -------
    SubwordTokenizer
    """

    def __init__(self, hash_file: str, do_lower_case: bool = True):

        self.do_lower_case = do_lower_case
        self.vocab_file = cpp_hashed_vocabulary(hash_file)

    def __call__(
        self,
        text,
        max_length,
        max_num_rows,
        add_special_tokens=True,
        padding="max_length",
        truncation=False,
        stride=0,
        return_tensors="cp",
        return_token_type_ids=False,
    ):
        """
        Run CUDA BERT subword tokenizer on cuDF strings column.
        Encodes words to token ids using vocabulary from a
        pretrained tokenizer.

        Parameters
        ----------
        text: (cudf String Series)
            The sequence or batch of sequences to be encoded.
            Each sequence can be a cudf string series

        max_length: (int)
            Controls the maximum length to use by one of
            the truncation/padding parameters.

        max_num_rows: (int)
            Maximum number of rows for the output token-ids expected to
            be generated by the tokenizer.
            Used for allocating temporary working memory on the GPU device.
            If the output generates a larger number of rows,
            behavior is undefined.
            This will vary based on stride, truncation, and max_length.
            For example, for non-overlapping sequences output rows will be
            the same as input rows.
            A good default can be twice the max_length

        add_special_tokens: (bool, optional, defaults to True)
            Whether or not to encode the sequences with the special tokens
            of the BERT classification model

        padding: ('max_length')
            Pad to a maximum length specified with the argument max_length

        truncation: (bool, defaults to False)
            True: Truncate to a maximum length specified with the
              argument max_length.
            False or 'do_not_truncate' (default)
              No truncation (Output differs from HuggingFace)

        stride (int, optional, defaults to 0)
            The value of this argument defines the number of
            overlapping tokens.
            The information about the overlapping tokens is
            present in the metadata outputed.

        return_tensors (str) default (cp.ndarray objects)
            'tf': Return TensorFlow tf.constant objects
            'pt': Return PyTorch torch.Tensor objects
            'cp': Return cupy cp.ndarray objects

        return_token_type_ids (bool, optional): Only False currently supported

        Returns
        -------
        A Encoding with the following fields:
        input_ids:(type defined by return_tensors)
            A tensor of token ids to be fed to a model.
        attention_mask: (type defined by return_tensors)
            A tensor of indices specifying which tokens
            should be attended to by the model
        metadata: (type defined by return_tensors)
            Each row contains the index id of the original string and the
            first and last index of the token-ids that are non-padded and
            non-overlapping

        Examples
        --------
        >>> import cudf
        >>> from cudf.utils.hash_vocab_utils  import hash_vocab
        >>> hash_vocab('bert-base-cased-vocab.txt', 'voc_hash.txt')

        >>> from cudf.core.subword_tokenizer import SubwordTokenizer
        >>> cudf_tokenizer  = SubwordTokenizer('voc_hash.txt',
                                                do_lower_case=True)

        >>> str_series = cudf.Series(['This is the', 'best book'])
        >>> tokenizer_output = cudf_tokenizer(str_series,
                            max_length=8,
                            max_num_rows=len(str_series),
                            padding='max_length',
                            return_tensors='pt',
                            truncation=True)
        >>>tokenizer_output['input_ids']
        tensor([[ 101, 1142, 1110, 1103,  102,    0,    0,    0],
                [ 101, 1436, 1520,  102,    0,    0,    0,    0]],
                device='cuda:0',
               dtype=torch.int32)
        >>>tokenizer_output['attention_mask']
        tensor([[1, 1, 1, 1, 1, 0, 0, 0],
                [1, 1, 1, 1, 0, 0, 0, 0]],
                device='cuda:0', dtype=torch.int32)

        >>>tokenizer_output['metadata']
        tensor([[0, 1, 3],
                  [1, 1, 2]], device='cuda:0', dtype=torch.int32)
       """

        if return_token_type_ids:
            # raise not currently supported
            # Can also return zeros
            error_msg = "Returning token_type_ids is currently supported"
            raise NotImplementedError(error_msg)

        if truncation in (False, "do_not_truncate"):
            truncation = False
            warning_msg = (
                "When truncation is not True, the behaviour currently differs "
                "from HuggingFace as cudf always returns overflowing tokens"
            )
            warn(warning_msg)
            if add_special_tokens:
                error_msg = (
                    f"Adding special tokens is not supported"
                    f"with truncation = {truncation}. "
                )
                recommendation = (
                    "Custom Cupy kernel can potentially"
                    "be used to add it. For reference "
                    "see: _bert_add_special_tokens "
                )
                raise NotImplementedError(error_msg + recommendation)

        if padding != "max_length":
            error_msg = (
                "Currently padding to the provided max_length is supported"
            )
            raise NotImplementedError(error_msg)

        if max_length <= stride:
            error_msg = "Stride should be less than max_length"
            raise ValueError(error_msg)

        if return_tensors not in ["cp", "pt", "tf"]:
            error_msg = (
                "Only cupy(cp), pytorch(pt) and tensorflow(tf)"
                "tensors are supported"
            )
            raise NotImplementedError(error_msg)

        stride = max_length - stride
        # behaviour varies from subword_tokenize but maps with huggingface

        input_ids, attention_mask, metadata = cpp_subword_tokenize(
            text._column,
            self.vocab_file,
            max_sequence_length=max_length,
            stride=stride,
            do_lower=self.do_lower_case,
            do_truncate=truncation,
            max_rows_tensor=max_num_rows,
        )

        tokenizer_output = dict()
        tokenizer_output["input_ids"] = cp.asarray(input_ids).reshape(
            -1, max_length
        )
        tokenizer_output["attention_mask"] = cp.asarray(
            attention_mask
        ).reshape(-1, max_length)
        tokenizer_output["metadata"] = cp.asarray(metadata).reshape(-1, 3)

        if add_special_tokens:
            tokenizer_output = _bert_add_special_tokens(tokenizer_output)

        tokenizer_output = {
            k: _cast_to_appripate_type(v, return_tensors)
            for k, v in tokenizer_output.items()
        }

        return tokenizer_output


def _bert_add_special_tokens(token_o):

    max_length = token_o["input_ids"].shape[1]
    seq_end_col = max_length - (token_o["input_ids"][:, ::-1] != 0).argmax(1)
    # clipping to take overflow into account
    seq_end_col = cp.clip(seq_end_col + 1, a_max=max_length - 1)

    _bert_add_special_tokens_input_ids(token_o["input_ids"], seq_end_col)
    _bert_add_special_tokens_attention_mask(
        token_o["attention_mask"], seq_end_col
    )
    _bert_add_spedical_tokens_metadata(token_o["metadata"], max_length)

    return token_o


def _bert_add_special_tokens_input_ids(input_ids, seq_end_col):
    # Mark sequence start with [CLS] token mapping to the start of sequence
    input_ids[:, 1:-1] = input_ids[:, 0:-2]
    input_ids[:, 0] = 101
    # Mark end of sequence [SEP]

    input_ids[
        cp.arange(0, input_ids.shape[0], dtype=cp.uint32), seq_end_col
    ] = 102


def _bert_add_special_tokens_attention_mask(attention_mask, seq_end_col):
    # Copy attention masks for all but last two
    attention_mask[:, 1:-1] = attention_mask[:, 0:-2]
    # Mark [CLS] token with 1
    attention_mask[:, 0] = 1
    # Mark [SEP] token with 1
    attention_mask[
        cp.arange(0, attention_mask.shape[0], dtype=cp.uint32), seq_end_col
    ] = 1


def _bert_add_spedical_tokens_metadata(metadata, max_length):
    # metadata seq starts from plus 1
    metadata[:, 1] = metadata[:, 1] + 1
    # clip done to take overflow into account
    metadata[:, 2] = cp.clip(metadata[:, 2] + 1, a_max=max_length - 2)
