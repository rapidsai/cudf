# Copyright (c) 2023-2025, NVIDIA CORPORATION.

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

import cudf
from cudf._fuzz_testing.utils import compare_dataframe


def test_parquet_long_list(tmpdir):
    # This test generates int and string list columns, where each has a row that is very large.
    # When generated by the cudf writer these long rows are contained on a single page,
    # but when generated by pyarrow they span several pages.
    # This checks that the parquet reader works properly for long rows spanning several pages.

    # Generate ranges
    small_int_range = range(1, 10)
    large_int_range = range(1, 100000)

    # Create int lists
    small_int_list = list(small_int_range)
    large_int_list = list(large_int_range)

    # Create string lists from the int ranges
    small_string_list = list(map(str, small_int_range))
    large_string_list = list(map(str, large_int_range))

    # Create arrays (columns) of these lists
    # The long rows start offset from the beginning of the page, span several pages,
    # and have another row following it on the last page.
    list_array_int = pa.array([small_int_list, large_int_list, small_int_list])
    list_array_string = pa.array(
        [small_string_list, large_string_list, small_string_list]
    )

    # Generate the table containing the columns
    generated_table = pa.Table.from_arrays(
        [list_array_string, list_array_int],
        names=["string_column", "int_column"],
    )

    # Write the table to a parquet file using pyarrow
    file_name = tmpdir.join("long_row_list_test.pq")
    # https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html
    pq.write_table(
        generated_table,
        file_name,
        use_dictionary=False,
        data_page_size=65536,
        version="1.0",
        write_page_index=False,
    )

    # Make sure that the cudf reader matches the pandas reader for this data
    actual = cudf.read_parquet(file_name)
    expected = pd.read_parquet(file_name)
    compare_dataframe(actual, expected, nullable=False)
