# Copyright (c) 2023-2025, NVIDIA CORPORATION.

from io import BytesIO

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import pytest

import cudf
from cudf.testing import assert_eq


def test_parquet_long_list(tmpdir):
    # This test generates int and string list columns, where each has a row that is very large.
    # When generated by the cudf writer these long rows are contained on a single page,
    # but when generated by pyarrow they span several pages.
    # This checks that the parquet reader works properly for long rows spanning several pages.

    # Generate ranges
    small_int_range = range(1, 10)
    large_int_range = range(1, 100000)

    # Create int lists
    small_int_list = list(small_int_range)
    large_int_list = list(large_int_range)

    # Create string lists from the int ranges
    small_string_list = list(map(str, small_int_range))
    large_string_list = list(map(str, large_int_range))

    # Create arrays (columns) of these lists
    # The long rows start offset from the beginning of the page, span several pages,
    # and have another row following it on the last page.
    list_array_int = pa.array([small_int_list, large_int_list, small_int_list])
    list_array_string = pa.array(
        [small_string_list, large_string_list, small_string_list]
    )

    # Generate the table containing the columns
    generated_table = pa.Table.from_arrays(
        [list_array_string, list_array_int],
        names=["string_column", "int_column"],
    )

    # Write the table to a parquet file using pyarrow
    file_name = tmpdir.join("long_row_list_test.pq")
    # https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html
    pq.write_table(
        generated_table,
        file_name,
        use_dictionary=False,
        data_page_size=65536,
        version="1.0",
        write_page_index=False,
    )

    # Make sure that the cudf reader matches the pandas reader for this data
    actual = cudf.read_parquet(file_name)
    expected = pd.read_parquet(file_name)
    assert actual.to_arrow().equals(pa.Table.from_pandas(expected))


@pytest.mark.parametrize(
    "index",
    [range(1, 11), list(range(1, 11)), range(1, 11)[::2]],
    ids=["RangeIndex", "IntIndex", "StridedRange"],
)
@pytest.mark.parametrize("write_index", [False, True, None])
@pytest.mark.parametrize("empty", [False, True], ids=["nonempty", "empty"])
def test_dataframe_parquet_roundtrip(index, write_index, empty):
    if empty:
        data = {}
    else:
        data = {"a": [i * 2 for i in index]}
    df = cudf.DataFrame(data=data, index=index)
    pf = pd.DataFrame(data=data, index=index)
    gpu_buf = BytesIO()
    cpu_buf = BytesIO()

    df.to_parquet(gpu_buf, index=write_index)
    pf.to_parquet(cpu_buf, index=write_index)
    gpu_table = pq.read_table(gpu_buf)
    cpu_table = pq.read_table(cpu_buf)
    metadata_equal = (
        gpu_table.schema.pandas_metadata == cpu_table.schema.pandas_metadata
    )
    assert metadata_equal

    gpu_read = cudf.read_parquet(gpu_buf)
    cpu_read = cudf.read_parquet(cpu_buf)
    assert_eq(gpu_read, cpu_read)
